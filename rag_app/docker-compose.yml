services:
  vespa:
    image: vespaengine/vespa:latest
    container_name: rag_vespa
    hostname: vespa
    ports:
      - "8080:8080"   # Vespa query + document API
      - "19071:19071" # Vespa config/metrics
    volumes:
      - ./vespa/app:/app:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:19071/state/v1/health >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30

  vespa-deployer:
    image: vespaengine/vespa:latest
    container_name: rag_vespa_deployer
    depends_on:
      vespa:
        condition: service_healthy
    volumes:
      - ./vespa/app:/app:ro
    restart: "no"
    entrypoint: ["/bin/bash", "-lc"]
    command:
      - |
        set -euo pipefail
        echo "[vespa-deployer] Deploying application package from /app to vespa:19071"
        /opt/vespa/bin/vespa-deploy -c vespa prepare /app
        /opt/vespa/bin/vespa-deploy -c vespa activate
        echo "[vespa-deployer] Done"

  ollama:
    image: ollama/ollama:latest
    container_name: rag_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama

  rag-api:
    build:
      context: ./rag-api
    container_name: rag_api
    environment:
      # Where to reach Ollama and Vespa *inside* the compose network
      - OLLAMA_BASE_URL=http://ollama:11434
      - VESPA_URL=http://vespa:8080

      # Models (you will pull these models once; see rag_app/README.md)
      - OLLAMA_CHAT_MODEL=llama3.1:8b
      - OLLAMA_EMBED_MODEL=nomic-embed-text

      # Embedding dimension MUST match the embedding model output AND the Vespa schema
      - EMBED_DIM=768

      # Vespa namespace + retrieval params
      - VESPA_NAMESPACE=my_ns
      - RAG_TOP_K=5
      - RAG_TARGET_HITS=50

      # Chunking defaults for ingestion
      - CHUNK_WORDS=220
      - CHUNK_OVERLAP_WORDS=40
    ports:
      - "8000:8000"
    depends_on:
      vespa:
        condition: service_healthy
      ollama:
        condition: service_started
      vespa-deployer:
        condition: service_completed_successfully

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: rag_openwebui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      # Optional: OpenWebUI can talk to Ollama directly.
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
      - rag-api

  vespa-metrics-exporter:
    build:
      context: ./vespa-metrics-exporter
    container_name: rag_vespa_metrics_exporter
    environment:
      - VESPA_METRICS_URL=http://vespa:19071/metrics/v2/values
      # Reduce cardinality by default: only export paths matching this regex (edit as needed)
      - EXPORT_PATH_REGEX=query|search|latency|feed|proton|hnsw|memory|cpu|http
    ports:
      - "9109:9109"
    depends_on:
      vespa:
        condition: service_healthy

  prometheus:
    image: prom/prometheus:latest
    container_name: rag_prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
    depends_on:
      - vespa-metrics-exporter

  grafana:
    image: grafana/grafana:latest
    container_name: rag_grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3001:3000"
    depends_on:
      - prometheus

volumes:
  ollama:
  open-webui:
  grafana:


